
"""
Melanie Weilert
September 2023
Purpose: Given a set of contribution scores and modisco models from tfmodiscolite, map hits based on designated CWM-scanning criteria.
Instructions:
1. Activate `bpreveal` conda environment.
2. `python /n/projects/mw2098/shared_code/bpreveal/tools/perform_cwm_scanning.py [options]`
3. For help: `python /n/projects/mw2098/shared_code/bpreveal/tools/perform_cwm_scanning.py -h`
"""
# Setup
import os
import sys
import re
import h5py
import json
import pandas as pd
import numpy as np
from tqdm import tqdm
from optparse import OptionParser
from joblib import Parallel, delayed
import numpy as np
from scipy.signal import correlate

sys.path.insert(0, f'/n/projects/mw2098/shared_code/bpreveal/functions/')
from stats import quantile_norm
from sliding_similarities import sliding_similarity

import warnings
warnings.filterwarnings("ignore")

os.environ['MKL_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_DYNAMIC'] = 'FALSE'

#Set up options
parser = OptionParser()
parser.add_option("-m", "--modisco_h5_path",
                  help="Path to the modisco.h5 file returned by tfmodisco-lite")
parser.add_option("-l", "--modisco_contrib_path",
                  help="Path to the contirbution .h5 file used by tfmodisco-lite model, generated by bpreveal")
parser.add_option("-c", "--mapping_contrib_path",
                  help="Path to the contirbution .h5 file designated for mapping, generated by bpreveal")
parser.add_option("-o", "--output_hits_path",
                  help="Output path for the tab-separated output file containing scanned hits.")
parser.add_option("-s", "--output_seqlets_path",
                  help="Output path for the tab-separated output file containing tfmodisco-lite seqlets.")

parser.add_option("-p", "--patterns_to_scan_json", default = None,
                  help="If None, scan all patterns. If an input .json file is entered, scan only patterns designated. JSON format is a dictionary of metacluster name --> [pattern names] [default: %default]")

parser.add_option("-f", "--mapping_filters", default = 'contrib_match_q>=.2,contrib_L1_q>=.01', type = "str",
                  help="String with comma-separated pd.df queries to filter which scanned sequence is a mapped motif instance. Can be anything, so long as it is compatible with the generated metadata. [default: %default]")
parser.add_option("-t", "--trim_threshold", default = .3, type = "float",
                  help="Threshold for tfmodisco-lite logo trimming. This default replicates the settings in tfmodisco-lite. [default: %default]")
parser.add_option("-r", "--trim_padding", default = 4, type = "int",
                  help="Padding added after trimming step. This default replicates the settings in tfmodisco-lite. [default: %default]")
parser.add_option("-w", "--modisco_window", default = 1000, type = "int",
                  help="Length of window to scan across, centered across each designated region of contribution. Recommended to keep consistent with tfmodisco-lite parameter. [default: %default]")
parser.add_option("-b", "--background_probs", default = '0.27,0.23,0.23,0.27', type = "str",
                  help="Background probabilities for PWM and PSSM calculations, designated as comma-separated string, ordered by ACGT. [default: %default]")
parser.add_option("-n", "--n_jobs", default = 12, type = "int",
                  help="Parallel workers for scanning logos. [default: %default]")
(options, args) = parser.parse_args()

def ppm_to_pwm(ppm, background_probs = [.25, .25, .25, .25]):
    pwm = np.log2(ppm/background_probs)
    return(pwm)

def ppm_to_pssm(ppm, background_probs = [.25, .25, .25, .25]):
        """Convert pwm array to pssm array
        pwm means that rows sum to one
        """
        ppm = ppm / ppm.sum(1, keepdims=True)
        ppm = ppm + 0.01  # add pseudo-counts
        ppm = ppm / ppm.sum(1, keepdims=True)
        b = np.array(background_probs)[np.newaxis]
        return np.log(ppm / b).astype(ppm.dtype)

def one_hot_decode_sequence(array):
    """
    Purpose: Given an array [position x 4], decode sequence to a string.
    """
    onehot_decoder = {
    0: 'A',
    1: 'C',
    2: 'G',
    3: 'T'
    }

    idxs = np.where(array)[1]
    return (''.join([onehot_decoder[i] for i in idxs]))


def bpreveal_cwm_scan(modisco_h5_path, modisco_contrib_path,
                      mapping_contrib_path,
                      output_hits_path, output_seqlets_path,
                      patterns_to_scan_json = None,
                      mapping_filters = 'contrib_match_q>=.2,contrib_L1_q>=.01',
                      trim_padding = 4,
                      trim_threshold = .3,
                      modisco_window = 1000,
                      background_probs = '0.27,0.23,0.23,0.27',
                      n_jobs = 12):

    # Make directories
    output_hits_dir = os.path.dirname(output_hits_path)
    output_seqlets_dir = os.path.dirname(output_seqlets_path)
    os.makedirs(output_hits_dir, exist_ok=True)
    os.makedirs(output_seqlets_dir, exist_ok=True)

    #Import .json file
    if patterns_to_scan_json is not None:
        with open(patterns_to_scan_json, "r") as file:
            patterns_to_scan_dict = json.load(file)

    #Process string-separated background probabilities
    background_probs = [float(b) for b in background_probs.split(',')]

    #Collect modisco metadata and contribution metadata
    modisco = h5py.File(modisco_h5_path, 'r')
    modisco_contrib = h5py.File(modisco_contrib_path, 'r')

    #Collect contribution scores of desired mapping regions
    mapping_contrib = h5py.File(mapping_contrib_path, 'r')
    contrib_scores = np.array(mapping_contrib['hyp_scores']) * np.array(mapping_contrib['input_seqs'])

    #Calculate contribution flank to pad scores with
    input_length = modisco_contrib['coords_end'][0] - modisco_contrib['coords_start'][0]
    input_flank = (input_length - modisco_window)//2

    #Collect contribution score coordinates for mapping genomic coordiantes of seqlets and instances
    modisco_contrib_coords_df = pd.DataFrame([
        np.array(modisco_contrib['coords_chrom'], dtype = 'str'),
        np.squeeze(modisco_contrib['coords_start']),
        np.squeeze(modisco_contrib['coords_end'])
    ]).transpose()
    modisco_contrib_coords_df.columns = ['chrom', 'peak_start', 'peak_end']
    modisco_contrib_coords_df['peak_idx'] = modisco_contrib_coords_df.index

    mapping_contrib_coords_df = pd.DataFrame([
        np.array(mapping_contrib['coords_chrom'], dtype = 'str'),
        np.squeeze(mapping_contrib['coords_start']),
        np.squeeze(mapping_contrib['coords_end'])
    ]).transpose()
    mapping_contrib_coords_df.columns = ['chrom', 'peak_start', 'peak_end']
    mapping_contrib_coords_df['peak_idx'] = mapping_contrib_coords_df.index

    hits_list = []
    seqlets_list = []

    for metacluster,patterns in modisco.items():
        if metacluster not in patterns_to_scan_dict.keys(): continue
        for pattern,metadata in patterns.items():
            if pattern not in patterns_to_scan_dict[metacluster]: continue
            print('Scanning ', metacluster, pattern)

            #Extract logos
            ppm = np.array(metadata['sequence'])
            pwm = ppm_to_pwm(ppm, background_probs = background_probs)
            pssm = ppm_to_pssm(ppm, background_probs = background_probs)
            cwm = np.array(metadata['contrib_scores'])

            #Recreate trimming steps of tfmodiscolite, slight change from original basepairmodels trimming
            #Kudos: https://github.com/jmschrei/tfmodisco-lite/blob/main/modiscolite/report.py
            cwm_score = np.sum(np.abs(cwm), axis=1)
            cwm_thresh = np.max(cwm_score) * trim_threshold
            pass_inds = np.where(cwm_score >= cwm_thresh)[0]
            start_boundary, end_boundary = max(np.min(pass_inds) - trim_padding, 0), min(np.max(pass_inds) + trim_padding + 1, len(cwm_score) + 1)

            motif_len = end_boundary - start_boundary

            cwm_trim = cwm[start_boundary:end_boundary]
            pssm_trim = pssm[start_boundary:end_boundary]
            pwm_trim = pwm[start_boundary:end_boundary]
            ppm_trim = ppm[start_boundary:end_boundary]

            #Scan SEQLETS, process values, and , IC scores
            #I.c. of each motif based on PSSM match (region x position[not padded])
            seqlet_ic_scan = sliding_similarity(pssm_trim, metadata['seqlets']['sequence'][:, start_boundary:end_boundary, :],
                                                metric = 'dotproduct', pad_mode = None, n_jobs = n_jobs, verbose = True)

            #Tuple of (jaccard score of the normalized array, L1 magnitude of the scanned window)
            seqlet_contrib_scan, seqlet_contrib_L1 = sliding_similarity(cwm_trim, metadata['seqlets']['contrib_scores'][:, start_boundary:end_boundary, :],
                                                                        metric='continousjaccard', pad_mode=None, n_jobs=n_jobs, verbose=True)

            #Establish distributions of ic_scores, contrib_match_scores, contrib_L1_scores
            #TODO: https://github.com/kundajelab/bpnet/blob/master/bpnet/modisco/core.py#L772
            #TODO: Charles, ask Anshul about this deviation from the published work.

            #Format seqlets to have coordinates and metadata
            seqlets_df = pd.DataFrame([
                np.squeeze(metadata['seqlets']['example_idx']),
                np.squeeze(metadata['seqlets']['start'])+ start_boundary,
                np.squeeze(metadata['seqlets']['start'])+ end_boundary,
                np.squeeze(metadata['seqlets']['is_revcomp']),
                [one_hot_decode_sequence(metadata['seqlets']['sequence'][i, start_boundary:end_boundary, :])
                 for i in range(metadata['seqlets']['sequence'].shape[0])],
                np.squeeze(seqlet_ic_scan),
                np.squeeze(seqlet_contrib_scan),
                np.squeeze(seqlet_contrib_L1)
            ]).transpose()
            seqlets_cols = ['fake_idx', 'peak_motif_start', 'peak_motif_end', 'strand', 'seq', 'ic_match', 'contrib_match', 'contrib_L1']
            seqlets_df.columns = seqlets_cols
            seqlets_df['strand'] = [ '-' if s else '+' for s in seqlets_df['strand'].values ]

            #TODO: Figure out how to connect 'fake_idx' to the original coordinate set.
            #TODO: Confirm sequences from seqlets are pre-oriented based on strand

            # seqlets_df = seqlets_df.merge(modisco_contrib_coords_df, how = 'left', on = ['peak_idx'])
            # seqlets_df['start'] = seqlets_df['peak_motif_start'] + seqlets_df['peak_start']
            # seqlets_df['end'] = seqlets_df['peak_motif_end'] + seqlets_df['peak_start']

            seqlets_df['pattern_idx'] = pattern
            seqlets_df['metacluster_idx'] = metacluster
            seqlets_df = seqlets_df[[#'chrom', 'start', 'end',
                                     'strand', 'seq', 'fake_idx', 'peak_motif_start', 'peak_motif_end',
                                     'metacluster_idx', 'pattern_idx', 'ic_match', 'contrib_match', 'contrib_L1']]
            # seqlets_df.to_csv('misc/test.tsv', sep = '\t', index = False)
            # print(pattern)
            seqlets_df['ic_match_q'] = np.array(quantile_norm(seqlets_df.ic_match.values, seqlets_df.ic_match.values))
            seqlets_df['contrib_match_q'] = np.array(quantile_norm(seqlets_df.contrib_match.values, seqlets_df.contrib_match.values))
            seqlets_df['contrib_L1_q'] = np.array(quantile_norm(seqlets_df.contrib_L1.values, seqlets_df.contrib_L1.values))

            print(pattern, ' scanning complete')
            # Collect seqlet thresholds based on quantile values
            seqlets_df['ic_match_q'] = seqlets_df['ic_match_q'].astype(float)
            seqlets_df['contrib_match_q'] = seqlets_df['contrib_match_q'].astype(float)
            seqlets_df['contrib_L1_q'] = seqlets_df['contrib_L1_q'].astype(float)
            seqlet_criteria_df = seqlets_df.copy().reset_index(drop = True)

            for f in mapping_filters.split(","):
                if len(f) > 0:
                    seqlet_criteria_df = seqlet_criteria_df.query(f)

            #Scan all contribution score windows provided
            #I.c. of each motif based on PSSM match (region x position[not padded])
            f_ic_scan = sliding_similarity(pssm_trim, mapping_contrib['input_seqs'], 'dotproduct', pad_mode = None, n_jobs = n_jobs, verbose = True)
            r_ic_scan = sliding_similarity(pssm_trim[::-1, ::-1], mapping_contrib['input_seqs'], 'dotproduct', pad_mode = None, n_jobs = n_jobs, verbose = True)
            ic_scan = np.stack([f_ic_scan, r_ic_scan], axis=-1)

            #Tuple of (jaccard score of the normalized array, L1 magnitude of the scanned window)
            #L1 = sum(abs(contrib))
            f_contrib_scan, contrib_L1 = sliding_similarity(cwm_trim, contrib_scores, metric='continousjaccard', pad_mode=None, n_jobs=n_jobs, verbose=True)
            r_contrib_scan, _ = sliding_similarity(cwm_trim[::-1, ::-1], contrib_scores, metric='continousjaccard', pad_mode=None, n_jobs=n_jobs, verbose=True)
            match_scan = np.stack([f_contrib_scan, r_contrib_scan], axis=-1)

            #Melanie's special jank-brew parsing that ONLY accounts for lower boundaries and ONLY the current parameters. Cheers, sorry.
            criteria_pass_list = []
            if mapping_filters.find('contrib_match')>-1:
                contrib_match_threshold = seqlet_criteria_df['contrib_match'].min()
                #If either positive or negative strand pass, keep
                contrib_match_scan_pass = np.any(match_scan>=contrib_match_threshold, axis = 2)
                criteria_pass_list.append(contrib_match_scan_pass)
            if mapping_filters.find('ic_match')>-1:
                ic_match_threshold = seqlet_criteria_df['ic_match'].min()
                #If either positive or negative strand pass, keep
                ic_match_scan_pass = np.any(ic_scan>=ic_match_threshold, axis = 2)
                criteria_pass_list.append(ic_match_scan_pass)
            if mapping_filters.find('contrib_L1')>-1:
                contrib_L1_threshold = seqlet_criteria_df['contrib_L1'].min()
                contrib_L1_scan_pass = contrib_L1>=contrib_L1_threshold
                criteria_pass_list.append(contrib_L1_scan_pass)

            #It must pass all specified criteria, keep indexes that pass true
            criteria_pass_arr = np.all(np.array(criteria_pass_list), axis = 0)
            criteria_pass_idx = np.where(criteria_pass_arr)

            #Take the criteria indexes, format into a data frame, then start adding in metadata
            criteria_pass_df = pd.DataFrame(criteria_pass_idx).transpose()
            criteria_pass_df.columns = ['peak_idx', 'peak_motif_start']
            criteria_pass_df['peak_motif_end'] = criteria_pass_df['peak_motif_start'] + motif_len
            # criteria_pass_df['seq'] = [one_hot_decode_sequence(mapping_contrib['input_seqs'][pi, pos:(pos + motif_len)])
            #                            for pi,pos in zip(criteria_pass_df.peak_idx.values, criteria_pass_df.peak_motif_start.values)]

            #Add coordinate information and modisco pattern information
            criteria_pass_df = criteria_pass_df.merge(mapping_contrib_coords_df, how = 'left', on = ['peak_idx'])
            criteria_pass_df['start'] = criteria_pass_df['peak_motif_start'] + criteria_pass_df['peak_start']
            criteria_pass_df['end'] = criteria_pass_df['peak_motif_end'] + criteria_pass_df['peak_start']
            criteria_pass_df['pattern_id'] = pattern
            criteria_pass_df['metacluster_id'] = metacluster

            # Add metadata
            criteria_pass_df['contrib_L1'] = [contrib_L1[pi, pos]
                                              for pi,pos in zip(criteria_pass_df.peak_idx.values, criteria_pass_df.peak_motif_start.values)]

            criteria_pass_df['contrib_L1'] = [contrib_L1[pi, pos]
                                              for pi,pos in zip(criteria_pass_df.peak_idx.values, criteria_pass_df.peak_motif_start.values)]
            criteria_pass_df['f_contrib_match'] = [match_scan[pi, pos, 0]
                                              for pi,pos in zip(criteria_pass_df.peak_idx.values, criteria_pass_df.peak_motif_start.values)]
            criteria_pass_df['r_contrib_match'] = [match_scan[pi, pos, 1]
                                              for pi,pos in zip(criteria_pass_df.peak_idx.values, criteria_pass_df.peak_motif_start.values)]
            criteria_pass_df['f_ic_match'] = [ic_scan[pi, pos, 0]
                                              for pi,pos in zip(criteria_pass_df.peak_idx.values, criteria_pass_df.peak_motif_start.values)]
            criteria_pass_df['r_ic_match'] = [ic_scan[pi, pos, 1]
                                              for pi,pos in zip(criteria_pass_df.peak_idx.values, criteria_pass_df.peak_motif_start.values)]

            #Select strand based on cwm match scores
            criteria_pass_df['contrib_match'] = np.max(np.stack([criteria_pass_df.f_contrib_match.values,
                                                                 criteria_pass_df.r_contrib_match.values]), axis = 0)
            criteria_pass_df['strand'] = ['+' if x==y else '-' for x,y in zip(criteria_pass_df.f_contrib_match.values,
                                                                              criteria_pass_df.contrib_match.values)]

            # #Consolidate any other columns that matter
            criteria_pass_df['ic_match'] = [x if z=='+' else y for x,y,z in zip(criteria_pass_df.f_ic_match.values,
                                                                                criteria_pass_df.r_ic_match.values,
                                                                                criteria_pass_df.strand.values)]

            #Get quantile values based on seqlet information
            criteria_pass_df['ic_match_q'] = np.array(quantile_norm(criteria_pass_df.ic_match.values, seqlets_df.ic_match.values))
            criteria_pass_df['contrib_match_q'] = np.array(quantile_norm(criteria_pass_df.contrib_match.values, seqlets_df.contrib_match.values))
            criteria_pass_df['contrib_L1_q'] = np.array(quantile_norm(criteria_pass_df.contrib_L1.values, seqlets_df.contrib_L1.values))

            #Re-implement criteria pass because of strand-specific redundancies
            #when doing the first, heavy filtering in np.arrays
            for f in mapping_filters.split(","):
                if len(f) > 0:
                    criteria_pass_df = criteria_pass_df.query(f)

            col_order = ['chrom', 'start', 'end', 'strand', 'peak_idx', 'metacluster_id', 'pattern_id',
                         'ic_match', 'contrib_match', 'contrib_L1', 'ic_match_q', 'contrib_match_q', 'contrib_L1_q',
                         'peak_start', 'peak_end', 'peak_motif_start', 'peak_motif_end']
            criteria_pass_df = criteria_pass_df[col_order]

            #Add to list of motifs
            hits_list.append(criteria_pass_df)
            seqlets_list.append(seqlets_df)

    hits_df = pd.concat(hits_list, axis = 0)
    seqlets_df = pd.concat(seqlets_list, axis = 0)

    hits_df.to_csv(output_hits_path, sep = '\t', index = False)
    seqlets_df.to_csv(output_seqlets_path, sep = '\t', index = False)
    return None

#Run featured function
bpreveal_cwm_scan(modisco_h5_path = options.modisco_h5_path,
                  modisco_contrib_path = options.modisco_contrib_path,
                  mapping_contrib_path = options.mapping_contrib_path,
                  output_hits_path = options.output_hits_path,
                  output_seqlets_path = options.output_seqlets_path,
                  patterns_to_scan_json = options.patterns_to_scan_json,
                  mapping_filters = options.mapping_filters,
                  trim_threshold = options.trim_threshold,
                  trim_padding = options.trim_padding,
                  modisco_window = options.modisco_window,
                  background_probs=  options.background_probs,
                  n_jobs = options.n_jobs)
